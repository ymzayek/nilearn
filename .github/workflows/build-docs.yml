# Workflow to build the documentation
name: Documentation builder

on:
  push:
    branches:
      - "main"
  pull_request:
    branches:
      - "*"
  schedule:
    - cron: "0 6 * * *"

jobs:
  restore_data:
    # This prevents this workflow from running on a fork.
    # To test this workflow on a fork, uncomment the following line.
    if: github.repository == 'ymzayek/nilearn'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout nilearn
        uses: actions/checkout@v3
        with:
          ref: ${{ github.event.pull_request.head.sha }}
      - name: Determine force download
        run: |
          commit_msg=$(git log -2 --format=oneline);
          echo $commit_msg;
          if [[ $commit_msg == *"[force download]"* ]]; then
            echo "All datasets will be downloaded as requested.";
            touch restore.txt;
          else
            echo "Data cache will be used if available.";
            echo "true" | tee restore.txt;
          fi
      - name: Get cache key
        run: |
          if [[ $(cat restore.txt) == "true" ]]; then
              date +%U > week_num;
          else
              echo "missing" > week_num;
          fi
      - name: Save data cache
        uses: actions/cache@v3
        with:
          path: |
            ~/nilearn_data/basc_multiscale_2015 # <  1 MB
            ~/nilearn_data/destrieux_surface    # <  1 MB
            ~/nilearn_data/difumo_atlases       # ~  4 MB
            ~/nilearn_data/fsaverage            # ~ 28 MB
            ~/nilearn_data/Megatrawls           # ~  9 MB
            ~/nilearn_data/msdl_atlas           # ~ 21 MB
            ~/nilearn_data/neurovault           # ~ 13 MB
            ~/nilearn_data/pauli_2017           # ~  1 MB
            ~/nilearn_data/yeo_2011             # ~  3 MB
          key: v1-small-cache-${{ hashFiles('**/week_num**') }}

  build_docs:
    # This prevents this workflow from running on a fork.
    # To test this workflow on a fork, uncomment the following line.
    if: github.repository == 'ymzayek/nilearn'
    runs-on: ubuntu-latest
    needs: restore_data
    defaults:
      run:
        shell: bash
    steps:
      - name: Restore source cache
        uses: actions/cache@v3
        with:
          path: .git
          key: source-cache-${{ runner.os }}-${{ github.run_id }}
          restore-keys: |
            source-cache-${{ runner.os }}
      - name: Checkout nilearn
        uses: actions/checkout@v3
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          fetch-depth: 0
      - name: Complete checkout
        run: |
          if ! git remote -v | grep upstream; then
            git remote add upstream https://github.com/ymzayek/nilearn.git
          fi
          git fetch upstream
      - name: Save source cache
        uses: actions/cache@v3
        with:
          path: .git
          key: source-cache-${{ runner.os }}-${{ github.run_id }}
      - name: Merge with upstream
        run: |
          echo $(git log -1 --pretty=%B) | tee gitlog.txt
          echo "gitlog.txt = $(cat gitlog.txt)"
          echo ${GITHUB_REF//*pull\//} | tee merge.txt
          if [[ $(cat merge.txt) != "" ]]; then
              echo "Merging $(cat merge.txt)";
              git pull --ff-only upstream "refs/pull/$(cat merge.txt)";
          fi
#      - name: Determine force download
#        run: |
#          commit_msg=$(git log -2 --format=oneline);
#          echo $commit_msg;
#          if [[ $commit_msg == *"[force download]"* ]]; then
#            echo "All datasets will be downloaded as requested.";
#            touch restore.txt;
#          else
#            echo "Data cache will be used if available.";
#            echo "true" | tee restore.txt;
#          fi
      - name: Setup conda
        uses: conda-incubator/setup-miniconda@v2
        with:
          auto-activate-base: true
          activate-environment: ''
          miniconda-version: 'latest'
          channels: conda-forge,defaults
      - name: Install apt packages
        run: |
          ./build_tools/github/build_dependencies_apt.sh
      - name: Install packages in conda env
        shell: bash -el {0}
        run: |
          ./build_tools/github/build_dependencies.sh
      - name: Find build type
        run: |
          ./build_tools/github/build_type.sh
      - name: Verify build type
        run: |
          echo "PATTERN = $(cat pattern.txt)"
          echo "BUILD = $(cat build.txt)"
#      - name: Get cache key
#        run: |
#          if [[ $(cat restore.txt) == "true" ]]; then
#              date +%U > week_num;
#          else
#              echo "missing" > week_num;
#          fi
#      - name: Restore data cache
#        id: restore-cache
#        uses: actions/cache@v3
#        with:
#          path: ~/nilearn_data
#          key: data-cache-${{ hashFiles('**/week_num**') }}
      - name: Build docs
        shell: bash -el {0}
        run: |
          source activate testenv;
          echo "Conda active env = $CONDA_DEFAULT_ENV";
          cd doc;
          set -o pipefail;
          PATTERN=$(cat ../pattern.txt) make $(cat ../build.txt) 2>&1 | tee log.txt;
      - name: Upload documentation
        uses: actions/upload-artifact@v3
        with:
          name: doc
          path: doc/_build/html
#      - name: Generate cache keys
#        run: |
#          date +%U > week_num;
#      - name: Save data cache
#        if: steps.restore-cache.outputs.cache-hit != 'true'
#        uses: actions/cache@v3
#        with:
#          path: ~/nilearn_data
#          key: data-cache-${{ hashFiles('**/week_num**') }}

  save_data:
    # This prevents this workflow from running on a fork.
    # To test this workflow on a fork, uncomment the following line.
    if: github.repository == 'ymzayek/nilearn'
    runs-on: ubuntu-latest
    needs: build_docs
    steps:
      - name: Generate cache keys
        run: |
          date +%U > week_num;
      - name: Save data cache
        uses: actions/cache@v3
        with:
          path: |
            ~/nilearn_data/basc_multiscale_2015 # <  1 MB
            ~/nilearn_data/destrieux_surface    # <  1 MB
            ~/nilearn_data/difumo_atlases       # ~  4 MB
            ~/nilearn_data/fsaverage            # ~ 28 MB
            ~/nilearn_data/Megatrawls           # ~  9 MB
            ~/nilearn_data/msdl_atlas           # ~ 21 MB
            ~/nilearn_data/neurovault           # ~ 13 MB
            ~/nilearn_data/pauli_2017           # ~  1 MB
            ~/nilearn_data/yeo_2011             # ~  3 MB
          key: v1-small-cache-${{ hashFiles('**/week_num**') }}